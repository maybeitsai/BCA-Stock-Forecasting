# -*- coding: utf-8 -*-
"""BCA-stock-forecasting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IFEeN7e6cGb-YhlDvcNqDE1YwseF4CrY

# Connect to Drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Import Library"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

"""# Data Loading"""

# Memuat data dari file CSV 'BBCA.JK.csv' ke dalam DataFrame
df = pd.read_csv('/content/drive/MyDrive/assets/BBCA.JK.csv')
df

"""# Exploratory Data Analysis (EDA)"""

# Menyiapkan DataFrame untuk analisis
df_analysis=df.copy()

# Menampilkan informasi umum tentang DataFrame
df_analysis.info()

# ubah date menjadi datetime
df_analysis['Date'] = pd.to_datetime(df_analysis['Date'])

# Menampilkan ringkasan statistik deskriptif dari DataFrame
df_analysis.describe()

# check missing values
df.isna().sum()

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
sns.pairplot(df_analysis, diag_kind = 'kde')

# Menghitung matriks korelasi antar fitur
correlation_matrix = df_analysis.corr()

# Membuat heatmap korelasi
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f",linewidths=0.5)
plt.title('Heatmap Korelasi Antar Fitur')
plt.show()

# Menghitung moving average volume perdagangan
df_analysis['Volume_MA'] = df_analysis['Volume'].rolling(window=20).mean()

# Visualisasi volume perdagangan dan moving average
plt.figure(figsize=(15, 6))
plt.plot(df_analysis['Date'], df_analysis['Volume'], label='Volume', color='blue')
plt.plot(df_analysis['Date'], df_analysis['Volume_MA'], label='Volume MA (20)', color='red', linestyle='--')
plt.title('Analisis Tren Volume Perdagangan Saham BCA')
plt.xlabel('Tanggal')
plt.ylabel('Volume Perdagangan')
plt.xticks(rotation=45)
plt.legend()
plt.show()

# Visualisasi tren harga saham BCA
plt.figure(figsize=(15, 6))
prices = ['Open', 'Close', 'High', 'Low', 'Adj Close']
colors = ['blue', 'red', 'green', 'orange', 'purple']

for price, color in zip(prices, colors):
    plt.plot(df_analysis['Date'], df_analysis[price], label=f'{price} Price', color=color)

plt.xticks(rotation=45)
plt.ylabel('Price')
plt.xlabel('Tahun')
plt.title("Analisis Tren Harga Saham BCA")
plt.legend()
plt.tight_layout()
plt.show()

# Menghitung moving average
df_analysis['MA_50'] = df_analysis['Close'].rolling(window=50).mean()
df_analysis['MA_200'] = df_analysis['Close'].rolling(window=200).mean()

# Visualisasi harga penutupan dan moving average
plt.figure(figsize=(18, 8))
plt.plot(df_analysis['Date'], df_analysis['Close'], label='Close Price', color='blue')
plt.plot(df_analysis['Date'], df_analysis['MA_50'], label='50-day MA', color='red', linestyle='--')
plt.plot(df_analysis['Date'], df_analysis['MA_200'], label='200-day MA', color='green', linestyle='--')
plt.title('Analisis Tren Harga Saham BCA')
plt.xlabel('Tahun')
plt.ylabel('Harga Penutupan')
plt.xticks(rotation=45)
plt.legend()
plt.show()

"""# Data Preprocessing"""

# Mengambil kolom Date dan Close dari DataFrame df_analysis
final_df=df_analysis[['Date','Close']]

# Mengambil kolom 'Close' dari DataFrame df_analysis sebagai kolom target
X = final_df['Close'].values.reshape(-1, 1)

# Memisahkan data training dan data testing
data_train, data_test = train_test_split(X,test_size=0.2,shuffle=False)

print(f"Jumlah data training : {data_train.shape}")
print(f"Jumlah data testing : {data_test.shape}")

# Melakukan normalisasi
sc = MinMaxScaler()
train_scaled = sc.fit_transform(data_train)
test_scaled = sc.fit_transform(data_test)

# Mempersiapkan data untuk pelatihan model
input_size = 100
def prepare_data(data, input_size):
    X, y = [], []
    for i in range(input_size, len(data)):
        X.append(data[i - input_size:i, 0])
        y.append(data[i, 0])

    X, y = np.array(X), np.array(y)
    X = np.reshape(X, (X.shape[0], X.shape[1], 1))
    return X, y

X_train, y_train = prepare_data(train_scaled, input_size)
X_test, y_test = prepare_data(test_scaled, input_size)

"""# Modeling

## LSTM Tensorflow
"""

pip install keras-tuner

# Import library Tensorflow dan kerastuner
import tensorflow as tf
from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,LSTM
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.metrics import RootMeanSquaredError
from kerastuner.tuners import RandomSearch

# Callback untuk membatasi proses pelatihan
class RMSEThresholdCallback(Callback):
    def __init__(self, threshold):
        super(RMSEThresholdCallback, self).__init__()
        self.threshold = threshold

    def on_epoch_end(self, epoch, logs={}):
        current_rmse = logs.get('root_mean_squared_error')
        current_val_rmse = logs.get('val_root_mean_squared_error')
        if (current_rmse <= self.threshold) and (current_val_rmse <= self.threshold) :
            print(f"\nEpoch {epoch + 1}: Training RMSE ({current_rmse:.4f}), Validation RMSE ({current_val_rmse:.4f}), does not exceed threshold ({self.threshold:.4f}). Stopping training.")
            self.model.stop_training = True

X_scaled = sc.fit_transform(X)
threshold_scaled_rmse = (X_scaled.max() - X_scaled.min()) * 1 / 100
rmse_threshold_callback = RMSEThresholdCallback(threshold_scaled_rmse)
print(f"Threshold scaled RMSE: {threshold_scaled_rmse:.4f}")

# Fungsi callback yang digunakan
callbacks = [
    EarlyStopping(monitor='root_mean_squared_error', patience=15, verbose=1, restore_best_weights=True),
    ModelCheckpoint(filepath='best_model.h5', monitor='root_mean_squared_error', save_best_only=True, verbose=1),
    ReduceLROnPlateau(factor=0.5, patience=5, min_lr=0.000001, verbose=1), #
    rmse_threshold_callback
]

# Membangun model LSTM
def build_model(hp):
    model = Sequential()
    model.add(LSTM(units=hp.Int('units', min_value=16, max_value=256, step=16),
                   return_sequences=True, input_shape=(X_train.shape[1], 1)))
    model.add(LSTM(units=hp.Int('units', min_value=16, max_value=256, step=16)))
    model.add(Dense(units=hp.Int('units', min_value=16, max_value=128, step=16), activation='relu'))
    model.add(Dense(units=hp.Int('units', min_value=8, max_value=64, step=8), activation='relu'))
    model.add(Dense(1))
    model.compile(optimizer=Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),
                  loss=MeanSquaredError(), metrics=[RootMeanSquaredError()])
    return model

"""### Hyperparameter Tuning"""

# Menggunakan kerastuner untuk menentukan hyperparameter terbaik
tuner = RandomSearch(
    build_model,
    objective='loss',
    max_trials=50,
    executions_per_trial=1,
    directory='my_dir',
    project_name='BCA_stock_forecasting')

tuner.search(X_train, y_train,
             epochs=10,
             batch_size=17,
             validation_data=(X_test, y_test))

# Mengambil model terbaik dari kerastuner
best_model_tf = tuner.get_best_models(num_models=1)[0]
best_hps = tuner.get_best_hyperparameters()[0]
best_model_tf.summary()

# Compile dan latih model terbaik
best_model_tf.compile(optimizer=Adam(learning_rate=best_hps.get('learning_rate')),
                   loss=MeanSquaredError(),
                   metrics=[RootMeanSquaredError()])

history_tf = best_model_tf.fit(X_train, y_train,
                         epochs=100,
                         validation_data=(X_test, y_test),
                         callbacks=callbacks,
                         batch_size=17)

# Plot RMSE
plt.figure(figsize=(12, 6))
plt.plot(history_tf.history['root_mean_squared_error'], label='Training RMSE')
plt.plot(history_tf.history['val_root_mean_squared_error'], label='Validation RMSE')
plt.title('Root Mean Squared Error (RMSE) on Training and Validation Sets')
plt.xlabel('Epoch')
plt.ylabel('RMSE')
plt.legend()
plt.show()

# Plot Loss
plt.figure(figsize=(12, 6))
plt.plot(history_tf.history['loss'], label='Training Loss')
plt.plot(history_tf.history['val_loss'], label='Validation Loss')
plt.title('Loss on Training and Validation Sets')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""## LSTM Pytorch"""

pip install optuna

# Import library pytorch dan optuna
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import optuna

# Membangun model LSTM
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMModel, self).__init__()
        # Membuat parameter LSTM
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # Membuat hidden state dan cell state
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

# Membuat fungsi evaluasi model
def evaluate_model(model, criterion, dataloader):
    model.eval()
    total_loss = 0.0
    with torch.no_grad():
        for inputs, targets in dataloader:
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            total_loss += loss.item() * len(inputs)
    return total_loss / len(dataloader.dataset)

# Membuat callback earlystopping
class EarlyStopping:
    def __init__(self, patience=10, verbose=False):
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = np.Inf

    def __call__(self, val_loss, model):
        score = -val_loss

        if self.best_score is None: # Simpan model pertama dan skornya
            self.best_score = score
            self.save_checkpoint(val_loss, model)
        elif score < self.best_score:  # Perbarui skor terbaik dan simpan model baru jika lebih baik
            self.counter += 1
            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else: # Skor tidak lebih baik, tetapi reset counter
            self.best_score = score
            self.save_checkpoint(val_loss, model)
            self.counter = 0

    def save_checkpoint(self, val_loss, model): # Checkpoint model
        if self.verbose:
            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')
        torch.save(model.state_dict(), 'checkpoint.pt')
        self.val_loss_min = val_loss

# Fungsi train model
def train_model(model, criterion, optimizer, train_loader, valid_loader, num_epochs):
    early_stopping = EarlyStopping(patience=5, verbose=True)
    for epoch in range(num_epochs):
        model.train()
        train_loss = 0.0
        for inputs, targets in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets.unsqueeze(1))
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * len(inputs)
        train_loss /= len(train_loader.dataset)

        valid_loss = evaluate_model(model, criterion, valid_loader)

        print(f'Epoch {epoch+1}/{num_epochs}, mse: {train_loss:.6f}, val_mse: {valid_loss:.6f}')

        if early_stopping.early_stop: # Berhenti jika early stopping aktif
            print("Early stopping")
            break

    print('Training finished')

def objective(trial):
    # Definisikan hyperparameter yang akan dioptimalkan
    hidden_size = trial.suggest_int('hidden_size', 16, 256, step=16)
    num_layers = trial.suggest_int('num_layers', 1, 4, step=1)
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)

    # Inisialisasi model dengan hyperparameter yang diusulkan
    model = LSTMModel(input_size=1, hidden_size=hidden_size, num_layers=num_layers, output_size=1)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # Training model
    num_epochs = 5
    batch_size = 42
    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)

     # Evaluasi model dengan data validasi
    valid_dataset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))
    valid_loader = DataLoader(valid_dataset, batch_size=batch_size)

    train_model(model, criterion, optimizer, train_loader, valid_loader, num_epochs)
    valid_loss = evaluate_model(model, criterion, valid_loader)

    return valid_loss

"""### Hyperparameter Tuning"""

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=50)

# Ambil hasil hyperparameter terbaik
best_params = study.best_params
print("Best Params:", best_params)

# Training model dengan seluruh data training
best_model_pytorch = LSTMModel(
    input_size=1,
    hidden_size=best_params['hidden_size'],
    num_layers=best_params['num_layers'],
    output_size=1)

optimizer= optim.Adam(best_model_pytorch.parameters(), lr=best_params['learning_rate'])
criterion = nn.MSELoss()

num_epochs = 100
batch_size = 42
train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)
valid_dataset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))
valid_loader = DataLoader(valid_dataset, batch_size=batch_size)

train_model(best_model_pytorch, criterion, optimizer, train_loader, valid_loader, num_epochs)

"""# Simulation prediction for next 30 days"""

# Mengambil 100 data terakhir dari kolom 'Close' dalam DataFrame final_df
data_demo = final_df['Close'].tail(100).reset_index(drop=True)

# Melakukan normalisasi data
data_demo_normalized = sc.transform(np.array(data_demo).reshape(-1, 1))

# Melakukan prediksi untuk 30 hari ke depan
lst_output = []
n_steps = 100
for i in range(30):
    if len(data_demo_normalized) > 100:
        X_demo = np.array(data_demo_normalized[-n_steps:]).reshape(1, -1, 1)
        yhat = best_model_tf.predict(X_demo, verbose=0)
        data_demo_normalized = np.append(data_demo_normalized, yhat)
        lst_output.extend(yhat.tolist())
    else:
        X_demo = np.array(data_demo_normalized).reshape(1, -1, 1)
        yhat = best_model_tf.predict(X_demo, verbose=0)
        data_demo_normalized = np.append(data_demo_normalized, yhat)
        lst_output.extend(yhat.tolist())

# Membuat array untuk visualisasi
day_new = np.arange(1, 101)
day_pred = np.arange(101, 131)
day_combined = np.arange(1,131)

# Menentukan rentang data aktual dan prediksi
actual_data = sc.inverse_transform(data_demo_normalized[:-30].reshape(-1, 1))
predicted_data = sc.inverse_transform(lst_output)

# Melakukan visualisasi data aktual dan prediksi
plt.plot(day_new, actual_data, label='Data Aktual', color='blue')
plt.plot(day_pred, predicted_data, label='Data Prediksi', color='red')
plt.title('Prediksi Harga Saham untuk 30 Hari ke Depan')
plt.xlabel('Hari')
plt.ylabel('Harga Penutup')
plt.legend()
plt.show()

# Menggabungkan data aktual dan prediksi menjadi satu array
combined_data = np.concatenate((actual_data, predicted_data), axis=0)

# Melakukan visualisasi data gabungan
plt.plot(day_combined, combined_data, label='Data Gabungan', color='blue')
plt.title('Prediksi Harga Saham untuk 30 Hari ke Depan')
plt.xlabel('Hari')
plt.ylabel('Harga Penutup')
plt.legend()
plt.show()